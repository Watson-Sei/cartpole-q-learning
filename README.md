# OpenAI Gym Cartpole Q-learning


https://github.com/Watson-Sei/cartpole-q-learning/assets/55475145/da4d4614-978d-4683-b5e7-f2c213106b51



## 処理ステップ
1. Q-Networkクラスの定義
- 全結合層（fc1, fc2, fc3)を定義
- 順伝播(forward)関数を定義

2. ハイパーパラメータを設定（学習率、割引率、エピソード数、最大ステップ数、ε-greedy法のパラメータ)
- 学習率：NNの重みを更新する際のステップサイズを制御し、学習率が大きすぎると、収束しない可能性があり小さすぎると局所最適化に陥る可能性があり、`0.001`が一般的な値
- 割引率(gamma): 将来の報酬をどの程度重視するかを決定する。割引率が1に近いほど、将来の報酬を重視し0に近いほど、直近の報酬を重視します。`0.99`は、将来の報酬を適度に考慮しつつ、収束性を維持するための一般的な値
- エピソード数(episodes): 学習を行うエピソードの総数を定義。エピソード数が多いほど、エージェントは環境について多くの経験を積むことができる。一方で、多すぎると学習時間がかかる。
- 最大ステップ数（max_steps): 最大ステップ数は、1エピソード内の最大ステップ数を定義。
- ε-greedy法パラメータ(epsilon, epsilon_decay, epsilon_min): 探索と活用のバランスを制御するパラメータ。εが大きいほど、ランダムな行動（探索）を取る確率が高くなり、小さいほどQ値に基づく行動（活用）を取る確率が高くなる。εを徐々に減少させることで、学習の初期は探索を重視し、後半は活用を重視するように。
    - epsilon_decay: εの減衰率を定義。0.995は、ゆっくりとεを減衰させるための一般的な値
    - epsilon_min: εの最小値を定義。0.01は、完全にランダムな行動を避けるための一般的な値

3. train関数の定義
- 環境から状態と行動の次元数を取得
- Q-Networkとオプティマイザ(Adam)を初期化
- 損失関数(MSE)を定義

4. 学習ループを開始
- 環境をリセットし、初期状態を取得
- ε-greedy法のεを更新する

5. エピソードループを開始
- ε-greedy法に基づいて行動を選択
- 選択した行動で環境をステップし、次の状態、報酬、終了フラグを取得
- カートの位置に基づく罰則を報酬に追加
- 次の状態での最大Q値を計算
- 現在の状態と行動のQ値を取得
- 損失を計算し、オプティマイザでQ-Networkを更新
- 状態を次の状態に更新
- エピソードの累積報酬を記録

6. エピソードループを終了
- エピソードの累積報酬、直近100エピソードの平均報酬と標準偏差を記録
- エピソードの情報を出力

7. 学習ループを終了
- 各エピソードの報酬をグラフ化して保存
- 直近100エピソードの平均報酬と標準偏差をグラフ化して保存

8. 学習済みのQ-Networkの重みを保存`.pth`


## 用語
### 全結合層
### ReLU活性化関数
### 損失関数（MSE）
### ε-greedy法
